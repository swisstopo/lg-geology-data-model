#!/usr/bin/env python3
import datetime
import json
import os
import subprocess
import sys
import importlib
import threading
from typing import Dict, Any, Optional
from pathlib import Path

import click
import pandas as pd
import pytz
import yaml
from jsonschema import Draft7Validator, ValidationError
from jsonschema import validate as jsonschema_validate
from loguru import logger
import traceback


def dynamic_import(module_name, attr_name):
    try:
        module = importlib.import_module(module_name)
        attr = getattr(module, attr_name)
        logger.debug(f"Successfully imported '{attr_name}' from '{module_name}'")
        return attr
    except ModuleNotFoundError:
        logger.warning(f"Module '{module_name}' not found.")
    except AttributeError:
        logger.warning(f"Attribute '{attr_name}' not found in '{module_name}'.")
    return None


ATTRIBUTES_TO_IGNORE = dynamic_import(
    "geocover.config", "ATTRIBUTES_TO_IGNORE"
) or dynamic_import("config", "ATTRIBUTES_TO_IGNORE")

Translator = dynamic_import("geocover.translator", "Translator") or dynamic_import(
    "translator", "Translator"
)


PACKAGE_NAME = "geocover"
EXPORTS_DIR = "exports"
# Source files, like SDE schema, tables exports, domains lists, etc.
SOURCES_DIR = yaml.safe_load(open("datamodel.yaml"))["model"]["sources_dir"]
# Directory for generated markdown files, used an `input` for `pandoc`
MARKDOWN_DIR = "inputs"
LOG_DIR = "log"

# Simplified version of the JSON generated by the Generate Schema Report geoprocessing tool
SIMPLIFIED_SDE_SCHEMA = "gcoverp_export_simple.json"

if not os.path.isdir(LOG_DIR):
    os.makedirs(LOG_DIR)

LOG_FILENAME = os.path.join(LOG_DIR, "datamodel.log")

logger.remove()
logger.add(sys.stdout, level="INFO")

if os.path.isfile(LOG_FILENAME):
    os.remove(LOG_FILENAME)
logger.add(LOG_FILENAME, backtrace=False, level="DEBUG")


"""with open(os.path.join(input_dir, "coded_domains.json"), "r") as f:
    domains = json.load(f)

with open(os.path.join(input_dir, "subtypes_dict.json"), "r") as f:
    subtypes = json.load(f)"""

"""json_struct_path = os.path.join(INPUT_DIR, "gcoveri_simple.json")
logger.info(f"Reading Schema from {json_struct_path}")
with open(
    json_struct_path,
    "r",
) as f:
    sde_schema = json.load(f)
    logger.info(sde_schema.keys())
    featclasses_dict = sde_schema.get("featclasses")
    tables_dict = sde_schema.get("tables")

    domains = sde_schema.get("coded_domain")
    subtypes = sde_schema.get("subtypes")
"""

"""
df_trad_load = pd.read_csv(
    os.path.join(input_dir, "GeolCodeText_Trad_230317.csv"), sep=";"
)

columns = ["GeolCode", "GeolCodeInt", "DE", "FR"]

# Load JSON data from a file
with open(
    os.path.join(input_dir, "all_codes_dict.json"), "r", encoding="utf-8"
) as file:
    code_dict = json.load(file)

df = pd.DataFrame(
    {
        "GeolCodeInt": list(code_dict.keys()),
        "DE": list(code_dict.values()),
        "FR": list(code_dict.values()),  # Initialize with None values
        "GeolCode": None,
    }
)


df["GeolCodeInt"] = df["GeolCodeInt"].astype("string")


merged_df = pd.concat([df_trad_load, df])
merged_df = merged_df.drop(columns=["GeolCode"], errors="ignore")

translation_xlsx_path = os.path.join(input_dir, "all_trads.xlsx")

df_trad = merged_df.drop_duplicates(subset=["GeolCodeInt"], keep="last")
df_trad.to_excel(translation_xlsx_path, index=False, engine="openpyxl")
df_trad["GeolCodeInt"] = df_trad["GeolCodeInt"].astype("string")
df_trad["DE"] = df_trad["DE"].astype("string")
df_trad["FR"] = df_trad["FR"].astype("string")

df.loc[df["GeolCodeInt"] == 0, ["DE", "FR"]] = ["-", "-"]

df_trad = df_trad.set_index(["GeolCodeInt"])
logger.info(f"Translation file has {len(df_trad)} translations")
logger.info(f"Saving file to {translation_xlsx_path} with {len(df_trad)} translations")
"""


def simplify_version(version_str):
    parts = version_str.split(".")
    if len(parts) >= 2:
        return ".".join(parts[:2])
    return version_str


class SDESchema:
    def __init__(self, json_path: str):
        self.json_path = Path(json_path)
        self._raw_data = self._load_json()
        self.featclasses = self._raw_data.get("featclasses", {})
        self.tables = self._raw_data.get("tables", {})
        self.coded_domains = self._raw_data.get("coded_domain", {})
        self.subtypes = self._raw_data.get("subtypes", {})

    def _load_json(self) -> Dict[str, Any]:
        """Load and validate JSON schema."""
        try:
            with open(self.json_path, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            raise ValueError(f"Schema file not found: {self.json_path}")
        except json.JSONDecodeError:
            raise ValueError(f"Invalid JSON in schema file: {self.json_path}")

    def get_table_fields(self, table_name: str) -> Dict[str, Any]:
        """Get all fields of a table."""
        return self.tables.get(table_name, {}).get("fields", {})

    def list_all_tables(self) -> list[str]:
        """Return all table names."""
        return list(self.tables.keys())

    def get_featclass_fields(self, featclass_name: str) -> Dict[str, Any]:
        """Get all fields of a table."""
        return self.featclasses.get(featclass_name, {}).get("fields", {})

    def list_all_featclasses(self) -> list[str]:
        """Return all feature classes names."""
        return list(self.featclasses.keys())

    def get_domain_values(self, domain_name: str) -> Optional[Dict[str, str]]:
        """Get coded domain values (e.g., {'1': 'Yes', '0': 'No'})."""
        return self.coded_domains.get(domain_name)


def get_sde_schema(source_dir):
    sde_schema = None
    try:
        schema_file_path = os.path.join(source_dir, SIMPLIFIED_SDE_SCHEMA)
        sde_schema = SDESchema(schema_file_path)
        logger.info(sde_schema.list_all_featclasses())
        logger.info(sde_schema.get_featclass_fields("TOPGIS_GC.GC_BEDROCK"))

        return sde_schema
    except ValueError as e:
        logger.error(f"Error loading schema from {schema_file_path}: {e}")
        return sde_schema


def load_translation_dataframe(input_dir: str, dataframes=[]) -> pd.DataFrame:
    """
    Load and merge translation data from CSV and JSON files, then return a cleaned translation DataFrame.

    Args:
        input_dir: Directory containing the input files

    Returns:
        pd.DataFrame: Translation dataframe with GeolCodeInt as index and DE/FR translations
    """
    # File paths
    # Inputs
    csv_path = os.path.join(input_dir, "GeolCodeText_Trad_230317.csv")
    # TODO: All new GMU codes found in the XLSX table (generations ?
    json_path = os.path.join(input_dir, "all_codes_dict.json")
    # New table  TODO use GeolCodeText_Trad_2025.xlsx with EN and IT
    new_translations_path = os.path.join(input_dir, "2025b_GeolCodeText_Trad.xlsx")
    # Custom Chrono CSV
    custom_chrono_path = os.path.join(input_dir, "geolcode_chrono.csv")

    # Translation only in templates, and trying to move translation from datamodel.yaml
    translation_xlsx_path = "translations.xlsx"
    # Ouputs
    xlsx_path = os.path.join(input_dir, "all_trads.xlsx")

    try:
        # Load application translations
        df_app_trad = pd.read_excel(
            translation_xlsx_path, usecols=["msg_id", "de", "fr"]
        )
        df_app_trad.columns = df_app_trad.columns.str.upper()
        df_app_trad.rename(columns={"MSG_ID": "GeolCodeInt"}, inplace=True)
        df_app_trad["GeolCodeInt"] = df_app_trad["GeolCodeInt"].astype("string")

        # Load Alan's CSV data
        df_trad_load = pd.read_csv(csv_path, sep=";")

        # Load new Alan's file
        df_new_trad = pd.read_excel(new_translations_path, sheet_name="Tabelle1")

        # Custom Chrono
        chrono_df = pd.read_csv(custom_chrono_path)

        # Load all Coded Domains data and convert to DataFrame
        with open(json_path, "r", encoding="utf-8") as file:
            code_dict = json.load(file)

        # Create DataFrame from JSON dict
        df_from_json = pd.DataFrame(
            {
                "GeolCodeInt": code_dict.keys(),
                "DE": code_dict.values(),
                "FR": code_dict.values(),  # Will be updated later if needed
            }
        )

        # Convert columns to string type
        df_from_json["GeolCodeInt"] = df_from_json["GeolCodeInt"].astype("string")
        df_from_json["DE"] = df_from_json["DE"].astype("string")
        df_from_json["FR"] = df_from_json["FR"].astype("string")

        logger.info(f"Number translations in 'translation.xlsx': {len(df_app_trad)} ")
        logger.info(f"Number translations in '{json_path}': {len(df_from_json)} ")
        logger.info(
            f"Number translations in 'GeolCodeText_Trad_230317.csv': {len(df_trad_load)} "
        )
        logger.info(
            f"Number translations in '{new_translations_path}': {len(df_new_trad)} "
        )

        new_rows = [
            {"GeolCodeInt": "999997", "DE": "unbekannt", "FR": "inconnu"},
            {"GeolCodeInt": "999998", "DE": "nicht anwendbar", "FR": "pas applicable"},
        ]
        special_code_df = pd.DataFrame(new_rows)

        # Merge DataFrames
        merged_df = pd.concat(
            dataframes
            + [
                df_from_json,
                df_trad_load,
                df_app_trad,
                df_new_trad,
                chrono_df,
                special_code_df,
            ]
        )

        merged_df["GeolCodeInt"] = merged_df["GeolCodeInt"].astype("string")

        # Clean and process the merged DataFrame
        df_trad = (
            merged_df.drop(
                columns=["GeolCode"], errors="ignore"
            )  # Remove unwanted column if exists
            .drop_duplicates(
                subset=["GeolCodeInt"], keep="last"
            )  # Keep last occurrence of duplicates
            .assign(
                DE=lambda x: x["DE"].replace("0", "-")
            )  # Replace '0' with '-' in DE
            .assign(
                FR=lambda x: x["FR"].replace("0", "-")
            )  # Replace '0' with '-' in FR
            .sort_values(by=["GeolCodeInt"])
            .set_index("GeolCodeInt")  # Set index
        )

        # Save to Excel

        df_trad.to_excel(xlsx_path, index=True, engine="openpyxl")

        logger.info(f"Translation file has {len(df_trad)} translations")
        logger.info(f"Saved translation file to {xlsx_path}")

        return df_trad

    except FileNotFoundError as e:
        logger.error(f"Input file not found: {e}")
        raise
    except json.JSONDecodeError as e:
        logger.error(f"Error parsing JSON file: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error loading translation data: {e}")
        raise


def get_datetime_with_tz():
    # Define the Western Europe timezone
    timezone = pytz.timezone("Europe/Zurich")
    tz_aware_datetime = datetime.datetime.now(timezone)

    return tz_aware_datetime.strftime("%Y-%m-%d %H:%M%z")


datetime_with_tz = get_datetime_with_tz()

po_header_tpl = f'''# Swiss Geology Datamodel
# Copyright (C) 2024 Free Software Foundation, Inc.
# This file is distributed under the same license as the {PACKAGE_NAME} package.
# Geocover Team <geocover@swisstopo.ch>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: {PACKAGE_NAME}"
"POT-Creation-Date: {datetime_with_tz}"
"PO-Revision-Date: {datetime_with_tz}"
"Last-Translator: geocover <geocover@swisstopo.ch>"
"Language-Team: LANGUAGE <LL@li.org>"
"Plural-Forms: nplurals=2; plural=(n != 1);"
"MIME-Version: 1.0"
"Content-Type: text/plain; charset=utf-8"
"Content-Transfer-Encoding: 8bit"'''


def get_git_revision_info():
    """Get git revision info and determine if this is a release version."""
    try:
        # Get short hash
        hash_short = (
            subprocess.check_output(["git", "rev-parse", "--short", "HEAD"])
            .decode("ascii")
            .strip()
        )

        # Check if current commit is tagged (indicates release)
        try:
            tag = (
                subprocess.check_output(
                    ["git", "describe", "--exact-match", "--tags", "HEAD"],
                    stderr=subprocess.DEVNULL,
                )
                .decode("ascii")
                .strip()
            )
            is_release = True
        except subprocess.CalledProcessError:
            is_release = False
            tag = None

        return {"hash": hash_short, "is_release": is_release, "tag": tag}
    except subprocess.CalledProcessError:
        return {"hash": "unknown", "is_release": False, "tag": None}


def get_git_revision_short_hash() -> str:
    """Backward compatibility function."""
    return get_git_revision_info()["hash"]


# TODO: to be removed
def create_msg(df):
    import polib

    de = list(zip(df["GeolCodeInt"], df["DE"]))

    fr = list(zip(df["GeolCodeInt"], df["FR"]))

    msgs = {}
    msgs["de"] = "\n".join([f'\nmsgid "{m[0]}"\nmsgstr "{m[1]}"' for m in de])
    msgs["fr"] = "\n".join([f'\nmsgid "{m[0]}"\nmsgstr "{m[1]}"' for m in fr])
    empty_pot = "\n".join([f'\nmsgid "{m[0]}"\nmsgstr ""' for m in fr])

    for lang in ("de", "fr"):
        locale_dir = f"./locale/{lang}/LC_MESSAGES"
        if not os.path.isdir(locale_dir):
            os.makedirs(locale_dir)
        po_file_path = os.path.join(locale_dir, "datamodel.po")
        with open(po_file_path, "w", encoding="utf-8") as f:
            f.write(po_header_tpl + msgs[lang])

        po = polib.pofile(po_file_path)
        # Remove duplicates while keeping the last occurrence
        unique_entries = {}
        for entry in po:
            unique_entries[entry.msgid] = (
                entry  # Dictionary ensures only the last occurrence is kept
            )
        # Create a new .po file
        new_po = polib.POFile()
        new_po.extend(unique_entries.values())
        new_po.save(po_file_path)

    with open(os.path.join("locale", "datamodel.pot"), "w", encoding="utf-8") as f:
        f.write(po_header_tpl + empty_pot)


# TODO: to be removed
# create_msg(df_trad)


# Custom sort key
def custom_sort_key(item):
    key, _ = item
    if key in ["999997", "999998"]:
        return float("inf")  # Ensure "999997" and "999998" come last
    return int(key)


def get_coded_values(domain_name, domains):
    if domain_name in domains.keys():
        domain = domains.get(domain_name)
        # print(f"  {name}, {att_type} , {domain_name}")
        if domain.get("type") == "CodedValue":
            coded_values = domain.get("codedValues")
            # TODO: hack
            if "999997" in coded_values.keys():
                coded_values["999997"] = "unbekannt"
            if "999998" in coded_values.keys():
                coded_values["999998"] = "nicht anwendbar"
            # Sort the list using the custom key
            return dict(sorted(coded_values.items(), key=custom_sort_key))

    return {}


def check_attribute_in_table(
    cls_name, table, attributes, abrev, prefixes, featclasses_dict
):
    ATTRIBUTES_TO_REMOVE = [
        "INTEGRATION_OBJECT_UUID",
        "MORE_INFO",
        "OBJECTID",
        "OBJECTORIGIN",
        "SYMBOL",
        "PRINTED",
    ] + ATTRIBUTES_TO_IGNORE

    model_attributes = []
    model_attributes_tuples = []

    prefixes = list(map(str.upper, prefixes))

    if abrev in prefixes:
        prefixes.remove(abrev)

    table_name = "TOPGIS_GC." + table.upper()

    table_dict = featclasses_dict.get(table_name)

    if table_dict:
        attributes_dict = featclasses_dict.get(table_name).get("fields")

    table_attributes = [d.get("name", "").upper() for d in attributes_dict]

    logger.debug(f"All prefixes: {prefixes}")

    for attribute in attributes:
        couple = []
        if attribute not in ATTRIBUTES_TO_REMOVE:
            couple.append(attribute.upper())
            model_attributes.append(attribute.upper())
            if attribute.lower() != "kind":
                attribute_name = (abrev + "_" + attribute).upper()
                couple.append(attribute_name)
            model_attributes_tuples.append(tuple(couple))

    # Ignore som common metadata columns
    all_table_attributes = [
        col for col in table_attributes if col not in ATTRIBUTES_TO_REMOVE
    ]

    # Ignore attributes whose prefix is not abrev
    table_attributes = [
        elem for elem in all_table_attributes if not elem.startswith(tuple(prefixes))
    ]

    # missing_in_table = [elem for tup in model_attributes_tuples if not any(e in table_attributes for e in tup) for elem in tup]

    logger.debug(f"Attributes in model {cls_name}: {model_attributes}")
    logger.debug(f"All attributes in feature class {table}: {all_table_attributes}")
    logger.debug(f"Filtered attributes in feature class {table}: {table_attributes}")

    if table_attributes:
        try:
            table_attributes_set = set(table_attributes)
            model_attributes_set = set(model_attributes)

            missing_in_model = sorted(list(table_attributes_set - model_attributes_set))
            missing_in_table = sorted(list(model_attributes_set - table_attributes_set))

            # missing_in_table = [elem for tup in model_attributes_tuples if not any(e in table_attributes for e in tup) for elem in tup]

            missing_in_table = [
                col for col in missing_in_table if col not in table_attributes
            ]

            missing_in_table = []

            for col in model_attributes:
                if (
                    col not in table_attributes
                    and (abrev + "_" + col).upper() not in table_attributes
                ):
                    missing_in_table.append(col)

        except Exception as e:
            logger.error(f"{e}")

        if len(missing_in_model) > 0:
            logger.warning(
                f"Class {cls_name} [{abrev}]: elements to add to model?: {missing_in_model}"
            )
        if len(missing_in_table) > 0:
            logger.warning(
                f"Class {cls_name} [{abrev}]: elements to remove from model? (not in feature class {table}): {missing_in_table}"
            )

    return (missing_in_model, missing_in_table)


def get_table_values(name):
    geol_dict = {}
    try:
        file_path = os.path.join(SOURCES_DIR, name)
        # df = pd.read_csv(file_path)

        with open(file_path, "r", encoding="utf-8") as f:
            geol_dict = json.load(f)

        """df = pd.DataFrame(data)

        # Convert to dictionary with 'GEOL_CODE_INT' as key and 'GERMAN' as value
        df["GEOL_CODE_INT"] = df["GEOL_CODE_INT"].astype(str)

        geol_dict = df.set_index("GEOL_CODE_INT")["GERMAN"].to_dict()"""

        return geol_dict

    except FileNotFoundError:
        logger.error(
            f"Error reading value for table {name}: The CSV file {file_path} was not found."
        )
    except pd.errors.EmptyDataError:
        logger.error(f"Error reading value for table {name}: The CSV file is empty.")
    except pd.errors.ParserError:
        logger.error(
            f"Error reading value for table {name}: There was a problem parsing the {file_path} file."
        )
    except Exception as e:
        logger.error(
            f"Error reading value for table {name}: An unexpected error occurred: {e}: {geol_dict}"
        )

    return {}


def get_subtype(subtypes, value):
    res = {}

    keys = [x for x in subtypes if str(x).startswith(str(value))]

    for key in keys:
        if key in subtypes.keys():
            value = subtypes.get(key)
            res[key] = value
    return res


def get_classes(model):
    classes = []
    for theme in model["themes"]:
        for cls in theme["classes"]:
            classes.append(cls.get("name"))
    return classes


def get_prefixes(model):
    prefixes = []
    for theme in model["themes"]:
        for cls in theme["classes"]:
            prefixes.append(cls.get("abrev"))
    return prefixes


class DatetimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime.date, datetime.datetime)):
            return obj.isoformat()
        return super().default(obj)


class Report:
    def __init__(self, config_file, sde_schema=None):
        self.config_file = config_file
        self._model = None
        self._prefixes = []
        self.sde_schema = sde_schema

    @property
    def model(self):
        if self._model:
            return self._model
        else:
            with open(self.config_file, "rt", encoding="utf8") as f:
                self._model = yaml.load(f, Loader=yaml.FullLoader)
                self._model["date"] = str(datetime.date.today())
                self._model["hash"] = get_git_revision_short_hash()
                self._model["git_info"] = get_git_revision_info()

            return self._model

    @property
    def prefixes(self):
        if len(self._prefixes) > 0:
            return self._prefixes
        prefixes = []
        for theme in self.model.get("themes", []):
            try:
                for cls in theme.get("classes", []):
                    cls_abrev = cls.get("abrev")
                    if cls_abrev:
                        prefixes.append(cls_abrev)
            except (KeyError, TypeError, IndexError) as e:
                logger.error(f"Error processing theme '{theme}': {e}")
                return None
        self._prefixes = sorted(list(set(prefixes)))
        return self._prefixes

    def to_json(self):
        try:
            model = self.model.copy()
        except AttributeError:
            logger.error("Model must be a dictionary-like object with a copy() method.")
            return None

        logger.info(f"All prefixes: {self.prefixes}")

        for theme in model.get("themes", []):
            try:
                theme_name = theme.get("name", "")
                if not theme_name:
                    logger.error(f"Missing 'name' attribute in theme '{theme_name}'.")
                    return None

                for cls in theme.get("classes", []):
                    cls_name = cls.get("name", "")
                    if not cls_name:
                        logger.error(f"Missing 'name' attribute in class '{cls_name}'")
                        return None
                    table_name = cls.get("table")
                    attributes = cls.get("attributes")
                    abrev = cls.get("abrev", "")
                    if abrev == "":
                        logger.error(f"Using default abrev: {cls_name}")
                        abrev = f"{theme['name'][0].upper()}{cls['name'][0:3].lower()}"
                    cls["abrev"] = abrev

                    if attributes:
                        attributes_in_model = []
                        for att in attributes:
                            att_type = att.get("att_type")
                            att_name = att.get("name")
                            value = att.get("value")
                            alias = att.get("alias")

                            pairs = None

                            if att_type == "CD" and value is not None:
                                pairs = get_coded_values(
                                    value, self.sde_schema.coded_domains
                                )

                            if att_type == "subtype" and value is not None:
                                pairs = get_subtype(
                                    self.sde_schema.subtypes, value
                                )  # TODO

                            # Short tables are treated like some CD
                            # TODO
                            if att_type == "table":
                                for annex in model.get("annexes"):
                                    if (
                                        att_name.lower()
                                        in annex.get("name", "").lower()
                                    ):  # TODO: not very robust
                                        annex_fname = annex.get("fname")
                                        logger.info(f" Founbd {annex_fname}")
                                        pairs = get_table_values(annex_fname)
                                        break

                            if pairs is not None:
                                att["pairs"] = pairs
                            if att.get("change", "") != "removed":
                                attributes_in_model.append(att_name)
                        try:
                            check_attribute_in_table(
                                cls_name,
                                table_name,
                                attributes_in_model,
                                cls["abrev"],
                                self.prefixes,
                                self.sde_schema.featclasses,
                            )
                        except Exception as e:
                            logger.error(
                                f"Theme: {cls_name}, attributes {attributes_in_model} not found in {table_name} check error: {e}"
                            )
                            logger.error(traceback.format_exc())

            except (KeyError, TypeError, IndexError) as e:
                logger.error(f"Error processing theme '{theme}': {e}")
                return None
        for annex in model.get("annexes"):
            annex_name = annex.get("name")

            if annex.get("type_") == "list":
                annex_fname = annex.get("fname")
                if annex_fname is not None:
                    pairs = get_table_values(annex_fname)

                else:
                    pairs = get_coded_values(annex_name, self.sde_schema.coded_domains)

                annex["pairs"] = pairs
            # table
            else:
                try:
                    json_data_path = os.path.join(SOURCES_DIR, annex.get("fname"))

                    with open(json_data_path, "r") as f:
                        data = json.loads(f.read())

                        annex["table"] = data

                except Exception as e:
                    logger.error(f"Error processing annexe '{annex_name}': {e}")

        return model


@click.group()
def datamodel():
    """Datamodel command group"""
    pass


@click.command()
@click.argument("datamodel", type=click.Path(exists=True))
def check():
    """Check the data model for consistency or errors."""
    click.echo("Checking data model...")


@click.command()
@click.argument("datamodel", type=click.Path(exists=True))
@click.option(
    "--output",
    "-o",
    type=click.Path(file_okay=True),
    default="inputs",
    help="Directory for output markdown files",
)
@click.option(
    "--format",
    "-f",
    type=click.Choice(["XLSX", "JSON"], case_sensitive=False),
    help="Directory for output markdown files",
    default="XLSX",
)
def export(datamodel, output, format):
    """Export model to various format."""

    from geocover import model

    yaml_path = datamodel

    datamodel = model.Datamodel()
    datamodel.import_from_yaml(yaml_path)
    logger.info(f"Export model to {format} '{output}")
    # datamodel.export_to_yaml("output.yaml")
    datamodel.export_to_excel(output)


@click.command(name="import")
@click.argument("datamodel", type=click.Path(exists=True))
@click.option(
    "--dryrun",
    "-n",
    help="Only test. Don't write",
    is_flag=True,
    default=True,
)
def import_model(datamodel, dryrun):
    """Import model from JSON or XLSX format."""

    from geocover import model

    xlsx_file = datamodel
    datamodel = model.Datamodel()

    logger.info("Import from  Excel")
    datamodel.import_from_excel(xlsx_file)

    logger.info("Export to YAML")

    datamodel.export_to_yaml("imported_model.yaml")


@click.command()
@click.argument("datamodel", type=click.Path(exists=True))
def validate(datamodel):
    """Validate the model against a JSON schema."""
    click.echo("Validate the data model...")
    try:
        with open(datamodel, "r", encoding="utf-8") as file:
            yaml_data = yaml.safe_load(file)
    except Exception as e:
        logger.error(f"Error while opening/parsing {datamodel}")
        sys.exit(3)

    version = yaml_data.get("version")
    if version:
        schema_file_path = os.path.join("schema", f"json-schema-{version}.json")
        try:
            with open(schema_file_path, "r", encoding="utf-8") as file:
                schema_data = json.load(file)
        except Exception as e:
            logger.error(f"Cannot load schemo {schema_file_path}")
            sys.exit(4)

    # Validate the loaded YAML data against the schema
    validator = Draft7Validator(schema_data)

    errors = sorted(validator.iter_errors(yaml_data), key=lambda e: e.path)
    if errors:
        for error in errors:
            error_path = " -> ".join(str(p) for p in error.path)
            logger.error(f"Validation error at {error_path}: {error.message}")
            error_instance = error.instance  # The actual value that caused the error

            # Log the error with additional context
            logger.error(f"Validation error at {error_path}: {error.message}")
            logger.error(f"Invalid value: {error_instance}")
    else:
        logger.info(f"YAML file {datamodel} is valid.")


@click.command()
@click.argument("datamodel", type=click.Path(exists=True))
def prettify(datamodel):
    """Prettifying the datamodel."""
    click.echo("Prettifying data model...")
    import sys

    import ruamel.yaml

    def block_style(base):
        """
        This routine walks over a simple, i.e. consisting of dicts, lists and
        primitives, tree loaded from YAML. It recurses into dict values and list
        items, and sets block-style on these.
        """
        if isinstance(base, dict):
            for k in base:
                try:
                    base.fa.set_block_style()
                except AttributeError:
                    pass
                block_style(base[k])
        elif isinstance(base, list):
            for elem in base:
                try:
                    base.fa.set_block_style()
                except AttributeError:
                    pass
                block_style(elem)

    yaml = ruamel.yaml.YAML()
    yaml.preserve_quotes = True
    try:
        with open(datamodel) as fp:
            data = yaml.load(fp)
        block_style(data)
        with open(datamodel, "w") as fp:
            yaml.dump(data, fp)
    except (FileNotFoundError, PermissionError):
        logger.error("You do not have permission to access this file.")
    except ruamel.yaml.YAMLError as e:
        logger.error(f"An error occurred while processing the YAML file: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")


@click.command("generate", context_settings={"show_default": True})
@click.option(
    "--lang",
    prompt="Language to generate",
    type=click.Choice(["de", "fr"], case_sensitive=False),
    help="Language for the document",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(file_okay=False),
    default=MARKDOWN_DIR,
    help="Directory for output markdown files",
)
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(file_okay=False),
    default=SOURCES_DIR,
    help="Directory for files sources (translation, codes, etc.)",
)
@click.argument("datamodel", type=click.Path(exists=True))
def generate(lang, datamodel, output, input_dir):
    """
    Generate a markdown document from the DATAMODEL YAML-file.
    """
    import datetime
    import os
    import re
    import sys
    from pathlib import Path

    import babel.dates
    import jinja2
    from babel import Locale
    from babel.core import Locale
    from babel.support import Translations
    from jinja2 import Template

    click.echo("Generating data model...")

    # Parsing
    Locale.negotiate(["de_DE", "en_US"], ["de_DE", "de_AT"])

    input_dir = os.path.abspath(input_dir)

    yaml_file = os.path.abspath(datamodel)
    yaml_dir = Path(yaml_file).parent
    if os.path.isabs(output):
        output_dir = output
    else:
        output_dir = Path(yaml_dir).joinpath(output)

    if not os.path.isdir(os.path.join(output_dir, lang)):
        os.makedirs(os.path.join(output_dir, lang))

    logger.info(f"Markdown files ouput dir: {output_dir}")

    project_name = Path(yaml_file).stem
    sde_schema = get_sde_schema(input_dir)
    featclasses_dict = sde_schema.featclasses
    tables_dict = sde_schema.tables
    domains = sde_schema.coded_domains
    subtypes = sde_schema.subtypes

    try:
        model = Report(yaml_file, sde_schema=sde_schema)

    except Exception as e:
        logger.error(f"Cannot read/parse datamodel: {yaml_file}: {e}")
        sys.exit()

    # Geolcode
    xlsx_file = os.path.join(yaml_dir, "exports", "comparison_results.xlsx")
    logger.info(f"comparison file: {xlsx_file}")
    added_rows = pd.read_excel(xlsx_file, sheet_name="Added Rows")
    removed_rows = pd.read_excel(xlsx_file, sheet_name="Removed Rows")
    changed_rows = pd.read_excel(xlsx_file, sheet_name="Changed Rows")
    # Convert DataFrames to dictionaries for Jinja2
    added_rows_dict = added_rows.to_dict(orient="records")
    removed_rows_dict = removed_rows.to_dict(orient="records")
    changed_rows_dict = changed_rows.to_dict(orient="records")

    classe_names = get_classes(model.model)
    prefixes = [p + " " for p in get_prefixes(model.model)]

    data = model.to_json()

    # TODO
    # print(data['annexes'][0])

    if data is None:
        raise RuntimeError("Model conversion to JSON failed, cannot proceed")
    now = datetime.datetime.now()

    data["geolcodes"] = {}
    data["geolcodes"]["added"] = added_rows_dict
    data["geolcodes"]["removed"] = removed_rows_dict
    data["geolcodes"]["changed"] = changed_rows_dict

    #
    with open(os.path.join(input_dir, "SCHEMA_CHANGES_4.0-4.1.json"), "r") as f:
        esri_diff_dict = json.load(f)
        data["changes"] = esri_diff_dict

    loader = jinja2.FileSystemLoader(os.path.join(yaml_dir, "templates"))
    env = jinja2.Environment(
        autoescape=True,
        loader=loader,
        extensions=["jinja2.ext.i18n"],
    )

    # TODO: remove Babel?
    locale_dir = os.path.join(yaml_dir, "locale")
    logger.info(locale_dir)
    mod_translations = Translations.load(locale_dir, [lang], "datamodel")
    translations = Translations.load(locale_dir, [lang], "app")

    # translations.merge(mod_translations)
    data["lang"] = lang
    data["date"] = now
    data["hash"] = get_git_revision_short_hash()
    data["model"]["short_revision"] = simplify_version(data["model"]["revision"])
    locale = lang

    # logger.info(json.dumps(data, indent=4,  cls=DatetimeEncoder))

    env.install_gettext_translations(translations, newstyle=True)

    # Custom filters

    def slugify(input):
        return re.sub(r"[\W_]+", "-", input.lower())

    def remove_prefix(value, prefixes=prefixes):
        for prefix in prefixes:
            if value.startswith(prefix):
                return value[len(prefix) :]
        return value

    def strip_final_dot(value):
        ori = value
        modified = value.strip().rstrip(".")
        if ori != modified:
            logger.debug(f"FOUND: {modified}")
        return modified

    # Define the custom filter function
    def format_date_locale(value, format="MMMM yyyy", locale="de_CH"):
        return babel.dates.format_date(date=value, format=format, locale=locale)

    def attribute_name(attribute):
        alias = attribute.get("alias")
        if alias:
            return alias
        return attribute.get("name")

    def highlight(input, words=classe_names, linkify=True):
        words.sort(key=len, reverse=True)  # longer first
        pattern = "({})".format(r"\b|\b".join(words))

        p = re.compile(pattern)
        if linkify:
            output = input
            if re.search(pattern, input) is not None:
                matches = re.finditer(p, input)

                for i, m in enumerate(matches):
                    word = m.group(1)
                    output = output.replace(
                        f" {word} ", f" [{word}](#{slugify(word)}) "
                    )
            return output

        return p.sub(r"**\1**", input)

    def get_domains_df(domains):
        geolcode = []
        labels = []

        for field_data in domains.values():
            if field_data.get("type") == "CodedValue":
                for k, v in field_data.get("codedValues", {}).items():
                    if int(k) > 1e6:
                        geolcode.append(k)
                        labels.append(v)

        # 🐼 Create DataFrame
        df = pd.DataFrame({"GeolCodeInt": geolcode, "DE": labels})

        return df

    # Used instead of babel
    try:
        geolcodes, descr = zip(*subtypes.items())
        geolcodes = list(geolcodes)
        descr = list(descr)
        subtypes_df = pd.DataFrame(
            {"GeolCodeInt": geolcodes, "DE": map(remove_prefix, descr)}
        )
        domains_df = get_domains_df(domains)

        df_translations = load_translation_dataframe(
            input_dir, dataframes=[subtypes_df, domains_df]
        )
        # Now you can use df_translations for your translations

    except Exception as e:
        # Handle error case
        print(f"Failed to load translations: {e}")

    print(df_translations.columns)
    translator = Translator(df_translations)

    def translate_filter(geol_code):
        """Translate geological code to default language."""
        return translator.translate(geol_code, lang=lang.upper())

    def translate_de_filter(geol_code):
        """Translate geological code to German."""
        return translator.translate(geol_code, lang="DE")

    def translate_fr_filter(geol_code):
        """Translate geological code to French."""
        return translator.translate(geol_code, lang="FR")

    # print(translator.translate('15001085', lang='DE'))
    # print(translate_filter('15001085'))

    env.filters["slugify"] = slugify
    env.filters["highlight"] = highlight
    env.filters["tr"] = translate_filter
    env.filters["tr_de"] = translate_de_filter
    env.filters["tr_fr"] = translate_fr_filter
    env.filters["format_date_locale"] = format_date_locale
    env.filters["remove_prefix"] = remove_prefix
    env.filters["attribute_name"] = attribute_name
    env.filters["strip_final_dot"] = strip_final_dot

    temp = env.get_template("model_markdown.j2")

    json_fname = os.path.join(output_dir, lang, f"{project_name}.json")
    logger.info(f"Generating JSON from model {json_fname}")
    with open(json_fname, "w", encoding="utf-8") as f:
        f.write(json.dumps(data, indent=4, cls=DatetimeEncoder))

    def render_template_with_locale(template_name, data, locale):
        template = env.get_template(template_name)
        return template.render(data, locale=locale)

    # DM
    markdown_fname = os.path.join(output_dir, lang, f"{project_name}.md")
    logger.info(f"Generating Markdown {markdown_fname}")
    with open(markdown_fname, "w", encoding="utf-8") as f:
        rendered = render_template_with_locale("model_markdown.j2", data, locale)
        f.write(rendered)

    # Changes to DM
    migration_dm_fname = os.path.join(output_dir, lang, f"{project_name}_migrations.md")
    logger.info(f"Generating changes/diffs to datamodel  {migration_dm_fname}")
    with open(migration_dm_fname, "w", encoding="utf-8") as f:
        rendered = render_template_with_locale("model_migrations.j2", data, locale)
        f.write(rendered)

    # Metadata
    metadata_fname = os.path.join(output_dir, lang, f"metadata.yaml")
    logger.info(f"Generating metadata  {metadata_fname}")
    with open(metadata_fname, "w", encoding="utf-8") as f:
        # f.write(template.render(data))
        rendered = render_template_with_locale("metadata.yaml.j2", data, locale)
        f.write(rendered)
    # HTML headers
    html_headers_fname = os.path.join(output_dir, lang, f"headers.html")
    logger.info(f"Generating HTML metadata {html_headers_fname}")
    with open(html_headers_fname, "w", encoding="utf-8") as f:
        # f.write(template.render(data))
        rendered = render_template_with_locale("headers.html.j2", data, locale)
        f.write(rendered)

    # Metadata
    changes_report_fname = os.path.join(
        output_dir, lang, f"changes_report_2022-2024.md"
    )
    logger.info(f"Generating change report {changes_report_fname}")
    with open(changes_report_fname, "w", encoding="utf-8") as f:
        # f.write(template.render(data))
        rendered = render_template_with_locale("changes_report.j2", data, locale)
        f.write(rendered)

    logger.info(
        f"Failed translations: {translator.get_translation_stats()['failed_translations']}"
    )


# Add the sub-commands to the main command group
datamodel.add_command(check)
datamodel.add_command(generate)
datamodel.add_command(prettify)
datamodel.add_command(validate)
datamodel.add_command(export)
datamodel.add_command(import_model)

if __name__ == "__main__":
    datamodel()
